{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocess import TextCleaner\n",
    "\n",
    "\n",
    "df = pd.read_pickle('../data/indeed_data.pkl')\n",
    "df = df.drop_duplicates(subset=['job_title', 'job_description'])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "custom_options = {\n",
    "    'remove_html': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_emails': True,\n",
    "    'remove_phone_numbers': True,\n",
    "    'remove_special_chars': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_extra_whitespace': True,\n",
    "    'remove_punctuation': False,\n",
    "    'convert_bullets': True,\n",
    "    'fix_sentence_spacing': True,\n",
    "    'lowercase': False,\n",
    "    'normalize_unicode': True\n",
    "}\n",
    "\n",
    "text_cleaner = TextCleaner(custom_options)\n",
    "df['job_description_clean'] = df['job_description'].apply(text_cleaner.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../data/usa_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. distill job descriptions\n",
    "\n",
    "- chunk job descriptions `chunk_job_postings.py` -> `data/usa_job_postings_chunks.pkl`\n",
    "- predict `train_classifier.py predict --model_path ../models/best_classifier --input_pickle data/usa_job_postings_chunks.pkl --output_pickle data/usa_job_postings_chunks_predictions.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(DATA_DIR / 'usa_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pickle.load(open(DATA_DIR / 'usa_job_postings_chunks_predictions.pkl', 'rb'))\n",
    "\n",
    "# Step 1: Filter for label == 1\n",
    "df_filtered = test_results[test_results['label'] == 1]\n",
    "\n",
    "# Step 2: Group by job_id and collect text\n",
    "result = df_filtered.groupby('job_id')['text'].apply(' '.join).reset_index()\n",
    "result.columns = ['job_id', 'text']\n",
    "\n",
    "df = pd.merge(df, result, on='job_id', how='left')\n",
    "\n",
    "# get the index of the rows where text is nan\n",
    "nan_indices = df[df['text'].isna()].index.tolist()\n",
    "\n",
    "# remove the rows where text is nan\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# save the dataframe\n",
    "df.to_pickle(DATA_DIR / \"usa_data_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. get embedding\n",
    "- `python get_embeddings.py --model_name \"Qwen/Qwen3-Embedding-8B\" --batch_size 32 --data_path \"../data/botswana_data_cleaned.pkl\" --output_path \"../data/botswana_data_embedding.pkl\" --device \"cuda:1\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = DATA_DIR / \"usa_data_embedding.pkl\"\n",
    "df = pickle.load(open(output_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "print(len(df_train), len(df_test))\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_train.to_pickle(DATA_DIR / \"usa_train_data_embedding.pkl\")\n",
    "df_test.to_pickle(DATA_DIR / \"usa_test_data_embedding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. check embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = np.vstack(df_train['title_qwen3_8b_emb'].values)\n",
    "title_embeddings_norm = np.linalg.norm(title_embeddings, axis=1, keepdims=True)\n",
    "title_embeddings = title_embeddings / title_embeddings_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "\n",
    "target_title_embedding = title_embeddings[idx]\n",
    "\n",
    "print(df_train.iloc[idx]['title'])\n",
    "\n",
    "scores = target_title_embedding @ title_embeddings.T\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "top_k_scores = scores[top_k_indices]\n",
    "\n",
    "top_k_titles = df_train.iloc[top_k_indices]['title'].tolist()\n",
    "\n",
    "top_k_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_embeddings = np.vstack(df_train['full_text_qwen3_8b_emb'].values)\n",
    "full_text_embeddings_norm = np.linalg.norm(full_text_embeddings, axis=1, keepdims=True)\n",
    "full_text_embeddings = full_text_embeddings / full_text_embeddings_norm\n",
    "\n",
    "\n",
    "embeddings = np.concatenate([title_embeddings, full_text_embeddings], axis=1)\n",
    "embeddings_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / embeddings_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "\n",
    "target_embedding = embeddings[idx]\n",
    "\n",
    "print(df_train.iloc[idx]['title'])\n",
    "\n",
    "scores = target_embedding @ embeddings.T\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "top_k_scores = scores[top_k_indices]\n",
    "\n",
    "top_k_titles = df_train.iloc[top_k_indices]['title'].tolist()\n",
    "\n",
    "top_k_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "df_train_deduplicated = df_train.dropna(subset=['title_qwen3_8b_emb', 'full_text_qwen3_8b_emb', 'description_qwen3_8b_emb']).reset_index(drop=True)\n",
    "\n",
    "title_embeddings = np.vstack(df_train_deduplicated['title_qwen3_8b_emb'].values)\n",
    "full_text_embeddings = np.vstack(df_train_deduplicated['full_text_qwen3_8b_emb'].values)\n",
    "text_embeddings = np.vstack(df_train_deduplicated['description_qwen3_8b_emb'].values)\n",
    "\n",
    "\n",
    "embeddings = np.concatenate([title_embeddings*0.8, text_embeddings*0.2], axis=1)\n",
    "embeddings_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / embeddings_norm\n",
    "\n",
    "symmetrized_similarities = embeddings @ embeddings.T\n",
    "\n",
    "af = AffinityPropagation(affinity='precomputed', random_state=0) # Use 'precomputed'\n",
    "af.fit(symmetrized_similarities)\n",
    "labels = af.labels_\n",
    "n_clusters_ = len(af.cluster_centers_indices_)\n",
    "print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "\n",
    "D = 1.01 - symmetrized_similarities\n",
    "np.fill_diagonal(D, 0)\n",
    "score = silhouette_score(D, labels, metric='precomputed')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out center examplars of the ap model\n",
    "titles = []\n",
    "for i in range(n_clusters_):\n",
    "    titles.append(df_train_deduplicated.iloc[af.cluster_centers_indices_[i]]['title'])\n",
    "titles.sort()\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Affinity propagation clustering with xgboost features\n",
    "- get the xboost model: \n",
    "    - first generate the training data using: same_occupation_job_pair_sampling.py\n",
    "    - then to train the xgboost model, run: same_occupation_classification.py\n",
    "- to get the clustering results, run clustering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "TMP_DIR = Path(\"../tmp\")\n",
    "CLUSTER_DIR = TMP_DIR / \"xgbt_clustering\"\n",
    "ALPHA = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 get training cluster id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(DATA_DIR / 'usa_train_data_embedding.pkl')\n",
    "df_test = pd.read_pickle(DATA_DIR / 'usa_test_data_embedding.pkl')\n",
    "\n",
    "embeddings = np.concatenate([np.vstack(df_train['title_qwen3_8b_emb'].values)*ALPHA, np.vstack(df_train['description_qwen3_8b_emb'].values)*(1-ALPHA)], axis=1)\n",
    "\n",
    "# qwen 8b\n",
    "model_path = CLUSTER_DIR / \"usa_ap_model_qwen3_8b.pkl\"\n",
    "af = pickle.load(open(model_path, 'rb'))\n",
    "    \n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "exemplars = embeddings[cluster_centers_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in range(len(af.cluster_centers_indices_)):\n",
    "    titles.append(df_train.iloc[af.cluster_centers_indices_[i]]['title'])\n",
    "titles.sort()\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k: v for k, v in enumerate(af.labels_)}\n",
    "print(len(id2label))\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a mapping from cluster labels to lists of IDs\n",
    "cluster2ids = defaultdict(list)\n",
    "for id_, cluster in id2label.items():\n",
    "    cluster2ids[cluster].append(id_)\n",
    "\n",
    "# Convert defaultdict to regular dict if needed\n",
    "cluster2ids = dict(cluster2ids)\n",
    "\n",
    "# Print the number of clusters and an example\n",
    "print(f\"Number of clusters: {len(cluster2ids)}\")\n",
    "print(f\"Example cluster contents (first cluster): {list(cluster2ids.values())[0][:5]}\")  # Show first 5 IDs of first cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cluster2ids\n",
    "pickle.dump(cluster2ids, open(CLUSTER_DIR / 'usa_cluster2id_mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 check cluster quality on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all jobs in cluster 1\n",
    "cluster_id = 99\n",
    "# print center of cluster 1\n",
    "print(df_train.iloc[af.cluster_centers_indices_[cluster_id]]['title'])\n",
    "print('-'*100)\n",
    "for i in range(len(af.labels_)):\n",
    "    if af.labels_[i] == cluster_id:\n",
    "        print(df_train.iloc[i]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 check test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "indices_to_check = np.arange(len(df_test))\n",
    "\n",
    "with open(TMP_DIR / \"usa_clustering_results_8b.txt\", \"w\") as f:\n",
    "    for idx in indices_to_check:\n",
    "        target_entry = df_test.iloc[idx]\n",
    "        target_embedding = np.concatenate([target_entry['title_qwen3_8b_emb']*ALPHA, target_entry['description_qwen3_8b_emb']*(1-ALPHA)], axis=1)[0]\n",
    "        \n",
    "        similarity_scores = cosine_similarity([target_embedding], embeddings)[0]\n",
    "\n",
    "        top_k = 10\n",
    "        top_k_indices = np.argsort(similarity_scores)[-top_k:][::-1]\n",
    "        top_k_titles = df_train.iloc[top_k_indices]['title'].tolist()\n",
    "        top_k_titles = '\\n'.join(top_k_titles)\n",
    "        \n",
    "        f.write(f\"JOB_ID: {target_entry['job_id']} ===\\n\")\n",
    "        f.write(f\"target title: {target_entry['title']}\\n\")\n",
    "        # f.write(f\"target description: {target_entry['job_description_full']}\\n\")\n",
    "        f.write(f\"top_k_titles: \\n{(top_k_titles)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the results\n",
    "with open(TMP_DIR / \"usa_clustering_results_8b.txt\", \"r\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Annotation with LLM (gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from LLM_annotation import annotate_occupations\n",
    "from get_prompts import get_truncated_prompts\n",
    "from prompts import instruction_v3\n",
    "\n",
    "client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get prompts\n",
    "\n",
    "df_train_embed = pd.read_pickle(DATA_DIR / 'usa_train_data_embedding.pkl')\n",
    "df_test_embed = pd.read_pickle(DATA_DIR / 'usa_test_data_embedding.pkl')\n",
    "\n",
    "# rename title to job_title\n",
    "df_train_embed.rename(columns={'title': 'job_title'}, inplace=True)\n",
    "\n",
    "# get prompts\n",
    "prompts = get_truncated_prompts(df_train_embed, root=None, k=15, use_mmr=True, lambda_param=0.5, instruction=instruction_v3, column_description='job_description', use_mapping='../tmp/xgbt_clustering/botswana_cluster2id_mapping.pkl')\n",
    "\n",
    "# save prompts\n",
    "pickle.dump(prompts, open(TMP_DIR / 'usa_prompts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. annotate occupations\n",
    "\n",
    "prompt_savepath = TMP_DIR / 'usa_gemini25_occupations.pkl'\n",
    "results = annotate_occupations(prompts, prompt_savepath, client, model=\"gemini-2.5-flash-preview-05-20\", temperature=0., max_tokens=4000, provider=\"google\")\n",
    "\n",
    "# save results\n",
    "pickle.dump(results, open(prompt_savepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def robust_json_parser(text: str):\n",
    "    \"\"\"\n",
    "    Extracts a JSON object from a string that might be wrapped in markdown code fences\n",
    "    or other text, and parses it.\n",
    "\n",
    "    Args:\n",
    "        text: The input string containing a JSON object.\n",
    "\n",
    "    Returns:\n",
    "        The parsed Python dictionary or list if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    # This regex finds a string that starts with '{' and ends with '}',\n",
    "    # and captures everything in between. re.DOTALL makes '.' match newlines.\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    \n",
    "    if not match:\n",
    "        # If no JSON object is found, try to parse the whole string\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: The string does not contain a valid JSON object.\")\n",
    "            return None\n",
    "\n",
    "    json_str = match.group(0)\n",
    "    \n",
    "    try:\n",
    "        # Parse the extracted string\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        # The extracted string is not valid JSON\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    try:\n",
    "        parsed_data = results[i]['annotation']\n",
    "        # print(parsed_data)\n",
    "    except:\n",
    "        print(i, results[i]['annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. normalize the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "import nltk # For lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet # For POS tagging\n",
    "\n",
    "\n",
    "# --- NLTK Setup (run once if not already downloaded) ---\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger.zip')\n",
    "except:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "except:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunctions = []\n",
    "gids = []\n",
    "misc = []\n",
    "all_titles = []\n",
    "for k, v in results.items():\n",
    "    if isinstance(v['annotation'], list):\n",
    "        title = v['annotation'][0]['occupation_title']\n",
    "    elif isinstance(v['annotation'], dict):\n",
    "        title = v['annotation']['occupation_title']\n",
    "    if '+' in title:\n",
    "        conjunctions.append(title)\n",
    "        gids.append(k)\n",
    "    if 'MISC' in title:\n",
    "        misc.append(title)\n",
    "    all_titles.append(title)\n",
    "print(len(conjunctions))\n",
    "for c in conjunctions:\n",
    "    print(c)\n",
    "print('-'*100)\n",
    "print(len(misc))\n",
    "for m in misc:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gids, open(TMP_DIR / 'usa_conjunction_cluster_ids.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idmapping = pickle.load(open(CLUSTER_DIR / 'usa_cluster2id_mapping.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunction_data = []\n",
    "for id in gids:\n",
    "    conjunction_data.extend(list(idmapping[id]))\n",
    "len(set(conjunction_data))\n",
    "\n",
    "# save the conjunction data\n",
    "pickle.dump(conjunction_data, open(TMP_DIR / 'usa_conjunction_data_ids.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove df train data\n",
    "df_train_embed = df_train_embed[~df_train_embed.index.isin(conjunction_data)]\n",
    "print(len(df_train_embed))\n",
    "df_train_embed.to_pickle(TMP_DIR / 'usa_train_data_no_conjunctions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_keep = {k:v for k,v in results.items() if k not in gids}\n",
    "len(results_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "for k, v in results_keep.items():\n",
    "    if isinstance(v['annotation'], list):\n",
    "        original_title = v['annotation'][0]['occupation_title']\n",
    "        all_titles.append(original_title)\n",
    "    elif isinstance(v['annotation'], dict):\n",
    "        title = v['annotation']['occupation_title']\n",
    "        all_titles.append(title)\n",
    "len(set(all_titles)),len(all_titles)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExaminerAgent:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        # Pre-compile regex for efficiency if called many times\n",
    "        self.camel_case_splitter = re.compile(r'(?<=[a-z0-9])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])')\n",
    "        self.punctuation_remover = re.compile(r'[^\\w\\s/-]')  # Keeps words, spaces, hyphens, and slashes\n",
    "        self.multiple_space_reducer = re.compile(r'\\s+')        \n",
    "\n",
    "    def _get_wordnet_pos(self, word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN) # Default to noun\n",
    "\n",
    "    def normalize_title(self, title):\n",
    "        if not title or not isinstance(title, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # 1. Insert spaces before uppercase letters in CamelCase/PascalCase\n",
    "        # e.g., \"OperationsCoordinator\" -> \"Operations Coordinator\"\n",
    "        # e.g., \"PDFReader\" -> \"PDF Reader\"\n",
    "        # e.g., \"MyAPITool\" -> \"My API Tool\"\n",
    "        title_spaced = self.camel_case_splitter.sub(r' ', title)\n",
    "\n",
    "        # 2. Convert to lowercase\n",
    "        title_lower = title_spaced.lower()\n",
    "        \n",
    "        # 3. Remove possessive 's\n",
    "        title_no_possessive = title_lower.replace(\"'s\", \"\")\n",
    "        \n",
    "        # 4. Remove common punctuation (keeps hyphens as they can be significant)\n",
    "        title_no_punct = self.punctuation_remover.sub('', title_no_possessive)\n",
    "        \n",
    "        # 5. Lemmatize\n",
    "        tokens = word_tokenize(title_no_punct) # Tokenize after most cleaning\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, self._get_wordnet_pos(token)) for token in tokens if token.strip()] # ensure no empty tokens\n",
    "\n",
    "        # 6. Join tokens and standardize multiple spaces to single, strip ends\n",
    "        normalized = ' '.join(lemmatized_tokens)\n",
    "        normalized = self.multiple_space_reducer.sub(' ', normalized).strip()\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "def capitalize_title(title):\n",
    "    skip_words = {'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', \n",
    "                'to', 'from', 'by', 'with', 'in', 'of'}\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        return title\n",
    "    # Always capitalize the first word\n",
    "    title_case = [words[0].capitalize()]\n",
    "    # For remaining words, capitalize unless they're in skip_words\n",
    "    for word in words[1:]:\n",
    "        if word not in skip_words:\n",
    "            title_case.append(word.capitalize())\n",
    "        else:\n",
    "            title_case.append(word)\n",
    "    return ' '.join(title_case)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examiner = ExaminerAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_input = []\n",
    "init_input_with_gid = []\n",
    "seen_titles = set()\n",
    "cluster2normalized_map = {}\n",
    "original2normalized_map = {}\n",
    "for k, v in results_keep.items():\n",
    "    if isinstance(v['annotation'], list):\n",
    "        original_title = v['annotation'][0]['occupation_title']\n",
    "        title = examiner.normalize_title(original_title)\n",
    "        title = capitalize_title(title)\n",
    "        original2normalized_map[original_title] = title\n",
    "        description = v['annotation'][0]['occupation_description']\n",
    "    elif isinstance(v['annotation'], dict):\n",
    "        title = v['annotation']['occupation_title']\n",
    "        original_title = title\n",
    "        title = examiner.normalize_title(title)\n",
    "        title = capitalize_title(title)\n",
    "        original2normalized_map[original_title] = title\n",
    "        description = v['annotation']['occupation_description']\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected annotation type: {type(v['annotation'])}\")\n",
    "    cluster2normalized_map[k] = title\n",
    "    if title in seen_titles:\n",
    "        continue\n",
    "\n",
    "    init_input.append({'title':title, 'description':description})\n",
    "    init_input_with_gid.append({'title':title, 'description':description, 'gid':k})\n",
    "    seen_titles.add(title)\n",
    "len(seen_titles), len(original2normalized_map), len(cluster2normalized_map), len(init_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "for i, d in enumerate(init_input):\n",
    "    title = d['title']\n",
    "    all_titles.append(title)\n",
    "for i in sorted(all_titles):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(original2normalized_map, open(TMP_DIR / 'usa_original2normalized_map.pkl', 'wb'))\n",
    "pickle.dump(cluster2normalized_map, open(TMP_DIR / 'usa_cluster2normalized_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_items = pd.DataFrame(init_input)\n",
    "df_original_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the description\n",
    "def get_embeddings(text, model, tokenizer, device='cuda:1'):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=4096)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    # Move output back to CPU for numpy conversion\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for description\n",
    "model_name = 'Qwen/Qwen3-Embedding-8B'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = model.to('cuda:1')\n",
    "\n",
    "batch_size = 32\n",
    "title_embeddings = []\n",
    "description_embeddings = []\n",
    "full_text_embeddings = []\n",
    "for i in tqdm(range(0, len(df_original_items), batch_size)):\n",
    "    batch = df_original_items.iloc[i:i+batch_size]\n",
    "                            \n",
    "    batch_titles = batch['title'].tolist()\n",
    "    batch_embeddings_title = [get_embeddings(text, model, tokenizer) for text in batch_titles]\n",
    "    title_embeddings.extend(batch_embeddings_title)\n",
    "\n",
    "    batch_descriptions = batch['description'].tolist()\n",
    "    batch_embeddings_description = [get_embeddings(text, model, tokenizer) for text in batch_descriptions]\n",
    "    description_embeddings.extend(batch_embeddings_description)\n",
    "\n",
    "    batch_full_texts = [title + '\\n' + description for title, description in zip(batch['title'], batch['description'])]\n",
    "    batch_embeddings_full_texts = [get_embeddings(text, model, tokenizer) for text in batch_full_texts]\n",
    "    full_text_embeddings.extend(batch_embeddings_full_texts)\n",
    "df_original_items['title_embeddings'] = title_embeddings\n",
    "df_original_items['description_embeddings'] = description_embeddings\n",
    "df_original_items['full_text_embeddings'] = full_text_embeddings\n",
    "df_original_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_titles = df_original_items['title'].tolist()\n",
    "description_embeddings = np.vstack(df_original_items['description_embeddings'].values)\n",
    "title_embeddings = np.vstack(df_original_items['title_embeddings'].values)\n",
    "embeddings = np.concatenate([description_embeddings, title_embeddings], axis=1)\n",
    "print(f\"Calculating similarity matrix for Affinity Propagation...\")\n",
    "similarity_matrix = sk_cosine_similarity(embeddings)\n",
    "\n",
    "# --- Clustering with Affinity Propagation ---\n",
    "PREFERENCE = 0.95\n",
    "DAMPING = 0.7\n",
    "RANDOM_STATE = 42 # For reproducibility\n",
    "\n",
    "\n",
    "ap_model = AffinityPropagation(\n",
    "    # damping=DAMPING,\n",
    "    preference=PREFERENCE,\n",
    "    affinity='precomputed',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "ap_model.fit(similarity_matrix)\n",
    "\n",
    "cluster_labels_for_valid = ap_model.labels_\n",
    "exemplar_indices_in_valid = ap_model.cluster_centers_indices_ # Indices within the 'valid_descriptions' list\n",
    "\n",
    "n_clusters_ = len(exemplar_indices_in_valid)\n",
    "print(f'Estimated number of clusters from Affinity Propagation: {n_clusters_}')\n",
    "\n",
    "# Create a mapping from cluster label to the canonical title (title of the exemplar)\n",
    "canonical_titles_map = {}\n",
    "for i, exemplar_idx_in_valid in enumerate(exemplar_indices_in_valid):\n",
    "    # exemplar_idx_in_valid is an index into 'description_embeddings' / 'original_titles_valid'\n",
    "    # The cluster label assigned by AP for this exemplar's cluster will be the exemplar's own index\n",
    "    # in 'description_embeddings' IF it's an exemplar.\n",
    "    # More robustly, AP assigns labels from 0 to n_clusters-1.\n",
    "    # We need to map which cluster label corresponds to which exemplar.\n",
    "    # The `ap_model.labels_` gives the cluster label for each point.\n",
    "    # The `exemplar_indices_in_valid` gives the index of the exemplar FOR EACH CLUSTER.\n",
    "    # A simpler way: ap_model.labels_[exemplar_idx_in_valid] gives the cluster label for that exemplar.\n",
    "    # We want: cluster_label -> title_of_exemplar_for_that_cluster\n",
    "    \n",
    "    # Let's find which cluster label corresponds to this exemplar\n",
    "    # The label of the cluster whose exemplar is at `exemplar_idx_in_valid`\n",
    "    # is simply `ap_model.labels_[exemplar_idx_in_valid]`\n",
    "    \n",
    "    cluster_label_of_exemplar = ap_model.labels_[exemplar_idx_in_valid]\n",
    "    canonical_titles_map[cluster_label_of_exemplar] = original_titles[exemplar_idx_in_valid]\n",
    "\n",
    "\n",
    "# Map cluster labels back to all original items\n",
    "all_cluster_labels = np.full(len(original_titles), -1, dtype=int) # Default to -1 (unclustered/error)\n",
    "final_canonical_titles = [None] * len(original_titles)\n",
    "\n",
    "for i in range(len(original_titles)):\n",
    "    original_item_cluster_label = cluster_labels_for_valid[i]\n",
    "    all_cluster_labels[i] = original_item_cluster_label\n",
    "    if original_item_cluster_label in canonical_titles_map:\n",
    "        final_canonical_titles[i] = canonical_titles_map[original_item_cluster_label]\n",
    "    else:\n",
    "        print(f\"Cluster label {original_item_cluster_label} not found in canonical_titles_map\")\n",
    "        # This case should ideally not happen if clustering is successful\n",
    "        # but as a fallback, use its own title if its cluster exemplar is not found (e.g. -1 label from fit)\n",
    "        final_canonical_titles[i] = original_titles[i]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "canonical2original_map = defaultdict(list)\n",
    "for i, v in enumerate(final_canonical_titles):\n",
    "    canonical2original_map[v].append(original_titles[i])\n",
    "for k, v in canonical2original_map.items():\n",
    "    if len(v) > 1:\n",
    "        print(k)\n",
    "        print(v)\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(canonical2original_map, open(TMP_DIR / 'usa_canonical2original_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original2canonical_map\n",
    "original2canonical_map = {}\n",
    "for k, v in canonical2original_map.items():\n",
    "    for vv in v:\n",
    "        original2canonical_map[vv] = k\n",
    "len(original2canonical_map)\n",
    "pickle.dump(original2canonical_map, open(TMP_DIR / 'usa_normalized2canonical_map.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the original titles to the canonical titles\n",
    "df_original_items['canonical_title'] = final_canonical_titles\n",
    "# save the dataframe\n",
    "df_original_items.to_pickle(TMP_DIR / 'usa_init_input_test_full_with_canonical_title_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2canonical_map = {}\n",
    "for k, v in cluster2normalized_map.items():\n",
    "    cluster2canonical_map[k] = original2canonical_map[v]\n",
    "len(cluster2canonical_map)\n",
    "pickle.dump(cluster2canonical_map, open(TMP_DIR / 'usa_cluster2canonical_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input format\n",
    "new_input = []\n",
    "title2description = {}\n",
    "for item in init_input:\n",
    "    title2description[item['title']] = item['description']\n",
    "for k in canonical2original_map.keys():\n",
    "    new_input.append({'title': k, 'description': title2description[k]})\n",
    "len(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(new_input, open(TMP_DIR / 'usa_init_input_test_full_canonical.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Tree generation\n",
    "\n",
    "`input: usa_init_input_test_full_canonical.json -> tree_multiagent.py -> output: full_generated_taxonomy_with_desc.json, level*_cannonical_parents_with_desc.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. convert to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TMP_DIR / 'usa_taxonomy_output/full_generated_taxonomy_with_desc.json', 'r') as f:\n",
    "    json_string = f.read()\n",
    "taxonomy_data_loaded = json.loads(json_string)\n",
    "len(taxonomy_data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_taxonomy_to_tree_format(taxonomy_data_loaded):\n",
    "    \"\"\"\n",
    "    Convert taxonomy data from the original format to the desired tree format.\n",
    "    Uses the title as the node name.\n",
    "    \n",
    "    Args:\n",
    "        taxonomy_data_loaded: Dictionary with level numbers as keys and lists of taxonomy items as values\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary representing the root node of the tree in the desired format\n",
    "    \"\"\"\n",
    "    # Create a mapping from title to node at each level\n",
    "    title_to_node = {}\n",
    "    \n",
    "    # Process levels from highest to lowest (e.g., 4, 3, 2, 1, 0)\n",
    "    max_level = max(int(k) for k in taxonomy_data_loaded.keys())\n",
    "    \n",
    "    # Start with the root node\n",
    "    root_node = {\n",
    "        \"parent\": \"null\",\n",
    "        \"name\": \"Root\",  # You can customize this root name\n",
    "        \"edge_name\": \"null\",\n",
    "        \"children\": []\n",
    "    }\n",
    "    \n",
    "    # Process each level starting from max_level down to 1\n",
    "    for level in range(max_level, 0, -1):\n",
    "        level_str = str(level)\n",
    "        if level_str not in taxonomy_data_loaded:\n",
    "            continue\n",
    "            \n",
    "        level_items = taxonomy_data_loaded[level_str]\n",
    "        \n",
    "        for i, item in enumerate(level_items):\n",
    "            title = item['title']\n",
    "            \n",
    "            # Create a node for this item\n",
    "            node = {\n",
    "                \"parent\": None,  # Will be set later\n",
    "                \"name\": title,\n",
    "                \"edge_name\": title,\n",
    "                \"children\": []\n",
    "            }\n",
    "            \n",
    "            # If this is the top level, attach directly to root\n",
    "            if level == max_level:\n",
    "                node[\"parent\"] = root_node[\"name\"]\n",
    "                root_node[\"children\"].append(node)\n",
    "            else:\n",
    "                # Find a parent for this node\n",
    "                # We'll look for a parent that has this title in its 'kids' list\n",
    "                found_parent = False\n",
    "                for potential_parent_level in range(level + 1, max_level + 1):\n",
    "                    parent_level_str = str(potential_parent_level)\n",
    "                    if parent_level_str not in taxonomy_data_loaded:\n",
    "                        continue\n",
    "                        \n",
    "                    for parent_item in taxonomy_data_loaded[parent_level_str]:\n",
    "                        if 'kids' in parent_item and title in parent_item['kids']:\n",
    "                            parent_node = title_to_node[parent_item['title']]\n",
    "                            node[\"parent\"] = parent_node[\"name\"]\n",
    "                            parent_node[\"children\"].append(node)\n",
    "                            found_parent = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_parent:\n",
    "                        break\n",
    "                \n",
    "                # If no parent found, attach to root (this is a fallback)\n",
    "                if not found_parent:\n",
    "                    node[\"parent\"] = root_node[\"name\"]\n",
    "                    root_node[\"children\"].append(node)\n",
    "            \n",
    "            # Store this node for future reference\n",
    "            title_to_node[title] = node\n",
    "    \n",
    "    # Process level 0 (leaf nodes) if available\n",
    "    if '0' in taxonomy_data_loaded:\n",
    "        for item in taxonomy_data_loaded['0']['Leaf Items']:\n",
    "            title = item['title']\n",
    "            leaf_node = {\n",
    "                \"parent\": None,\n",
    "                \"name\": title,\n",
    "                \"edge_name\": \"\",  # Using the format from your example\n",
    "                \"children\": []\n",
    "            }\n",
    "            \n",
    "            # Find a parent for this leaf node\n",
    "            found_parent = False\n",
    "            for level in range(1, max_level + 1):\n",
    "                level_str = str(level)\n",
    "                if level_str not in taxonomy_data_loaded:\n",
    "                    continue\n",
    "                    \n",
    "                for parent_item in taxonomy_data_loaded[level_str]:\n",
    "                    if 'kids' in parent_item and title in parent_item['kids']:\n",
    "                        parent_node = title_to_node[parent_item['title']]\n",
    "                        leaf_node[\"parent\"] = parent_node[\"name\"]\n",
    "                        parent_node[\"children\"].append(leaf_node)\n",
    "                        found_parent = True\n",
    "                        break\n",
    "                \n",
    "                if found_parent:\n",
    "                    break\n",
    "            \n",
    "            # If no parent found, attach to root (this is a fallback)\n",
    "            if not found_parent:\n",
    "                leaf_node[\"parent\"] = root_node[\"name\"]\n",
    "                root_node[\"children\"].append(leaf_node)\n",
    "    \n",
    "    return root_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = convert_taxonomy_to_tree_format(taxonomy_data_loaded)\n",
    "# save the result\n",
    "# Write the output tree to a JSON file.\n",
    "with open(\"../docs/usa_Gemini_generated_tree.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
