{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024/9/1 - 2025/2/14 jobs.ps\n",
    "data = pickle.load(open(DATA_DIR / 'palestine_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn into dataframe, only keep the columns we need: job_id, title, job_description, job_requirements\n",
    "# turn dictionary into dataframe, key as the index, value as the column\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.head()\n",
    "# only keep the columns we need: job_id, title, job_description, job_requirements\n",
    "df = df[['job_id', 'title', 'job_description', 'job_requirements']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats length of job_description as word count\n",
    "df['job_description'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job_requirements'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine job_description and job_requirements\n",
    "df['job_description_full'] = df['job_description'] + '\\n' + df['job_requirements']\n",
    "df['job_description_full'].apply(lambda x: len(x.split())).describe()\n",
    "# 1. get embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocess import TextCleaner\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "df['job_description_full_cleaned'] = df['job_description_full'].apply(cleaner.clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job_description_full_cleaned'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. get embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:1'\n",
    "MODEL_NAME = 'Qwen/Qwen3-Embedding-8B'\n",
    "MAX_LENGTH = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    # Move output back to CPU for numpy conversion\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import warnings\n",
    "\n",
    "def get_embeddings_flash_attention(texts, model, tokenizer, max_length=4096, batch_size=1):\n",
    "    \"\"\"\n",
    "    Get embeddings using FlashAttention2 with proper padding handling.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings or single text string\n",
    "        model: Pre-loaded model\n",
    "        tokenizer: Pre-loaded tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # For FlashAttention2, we need to handle padding carefully\n",
    "        # Option 1: Process each text individually (no padding issues)\n",
    "        if hasattr(model.config, '_attn_implementation') and model.config._attn_implementation == 'flash_attention_2':\n",
    "            batch_embeddings = []\n",
    "            for text in batch_texts:\n",
    "                # Process individually to avoid padding issues\n",
    "                inputs = tokenizer(text, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    # Use mean pooling over the sequence dimension\n",
    "                    embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "                    batch_embeddings.append(embedding)\n",
    "            \n",
    "            batch_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "        else:\n",
    "            # Standard batched processing with padding\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                             return_tensors='pt', max_length=max_length)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "def load_model_with_fallback(model_name, device='cuda', try_flash_attention=True):\n",
    "    \"\"\"\n",
    "    Load model with FlashAttention2 if available, otherwise fallback to standard attention.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if try_flash_attention:\n",
    "        try:\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_name, \n",
    "                attn_implementation=\"flash_attention_2\", \n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            print(\"‚úÖ Successfully loaded model with FlashAttention2\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  FlashAttention2 not available: {e}\")\n",
    "            print(\"üì¶ Install with: pip install flash-attn --no-build-isolation\")\n",
    "            print(\"üîÑ Falling back to standard attention...\")\n",
    "            model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = DATA_DIR / \"palestine_train_test_data_embedding.pkl\"\n",
    "\n",
    "if not output_path.exists():\n",
    "# if True:\n",
    "    model_name = MODEL_NAME\n",
    "\n",
    "    model, tokenizer = load_model_with_fallback(MODEL_NAME, DEVICE, try_flash_attention=True)\n",
    "    # tokenizer.padding_side  = 'left'\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    batch_size = 32\n",
    "    title_embeddings = []\n",
    "    description_embeddings = []\n",
    "    full_text_embeddings = []\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "                                \n",
    "        batch_titles = batch['title'].tolist()\n",
    "        batch_embeddings_title = [get_embeddings_flash_attention(text, model, tokenizer, max_length=MAX_LENGTH) for text in batch_titles]\n",
    "        title_embeddings.extend(batch_embeddings_title)\n",
    "\n",
    "        batch_descriptions = batch['job_description_full_cleaned'].tolist()\n",
    "        batch_embeddings_description = [get_embeddings_flash_attention(text, model, tokenizer, max_length=MAX_LENGTH) for text in batch_descriptions]\n",
    "        description_embeddings.extend(batch_embeddings_description)\n",
    "\n",
    "        batch_full_texts = [title + '\\n' + description for title, description in zip(batch['title'], batch['job_description_full_cleaned'])]\n",
    "        batch_embeddings_full_texts = [get_embeddings_flash_attention(text, model, tokenizer, max_length=MAX_LENGTH) for text in batch_full_texts]\n",
    "        full_text_embeddings.extend(batch_embeddings_full_texts)\n",
    "        \n",
    "    df['title_qwen3_8b_emb'] = title_embeddings\n",
    "    df['description_qwen3_8b_emb'] = description_embeddings\n",
    "    df['full_text_qwen3_8b_emb'] = full_text_embeddings\n",
    "    print(df.head(1))\n",
    "    df.to_pickle(output_path)\n",
    "df = pickle.load(open(output_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_train.to_pickle(DATA_DIR / \"palestine_train_data_embedding.pkl\")\n",
    "df_test.to_pickle(DATA_DIR / \"palestine_test_data_embedding.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. check embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title_embeddings = np.vstack(df_train['title_qwen3_8b_emb'].values)\n",
    "title_embeddings_norm = np.linalg.norm(title_embeddings, axis=1, keepdims=True)\n",
    "title_embeddings = title_embeddings / title_embeddings_norm\n",
    "\n",
    "idx = 4\n",
    "\n",
    "target_title_embedding = title_embeddings[idx]\n",
    "\n",
    "print(df_train.iloc[idx]['title'])\n",
    "\n",
    "scores = target_title_embedding @ title_embeddings.T\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "top_k_scores = scores[top_k_indices]\n",
    "\n",
    "top_k_titles = df_train.iloc[top_k_indices]['title'].tolist()\n",
    "\n",
    "top_k_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_text_embeddings = np.vstack(df_train['full_text_qwen3_8b_emb'].values)\n",
    "full_text_embeddings_norm = np.linalg.norm(full_text_embeddings, axis=1, keepdims=True)\n",
    "full_text_embeddings = full_text_embeddings / full_text_embeddings_norm\n",
    "\n",
    "\n",
    "embeddings = np.concatenate([title_embeddings, full_text_embeddings], axis=1)\n",
    "embeddings_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / embeddings_norm\n",
    "idx = 4\n",
    "\n",
    "target_embedding = embeddings[idx]\n",
    "\n",
    "print(df_train.iloc[idx]['title'])\n",
    "\n",
    "scores = target_embedding @ embeddings.T\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "top_k_scores = scores[top_k_indices]\n",
    "\n",
    "top_k_titles = df_train.iloc[top_k_indices]['title'].tolist()\n",
    "\n",
    "top_k_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = np.vstack(df_train['description_qwen3_8b_emb'].values)\n",
    "text_embeddings_norm = np.linalg.norm(text_embeddings, axis=1, keepdims=True)\n",
    "text_embeddings = text_embeddings / text_embeddings_norm\n",
    "\n",
    "\n",
    "embeddings = np.concatenate([title_embeddings*0.8, text_embeddings*0.2], axis=1)\n",
    "embeddings_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / embeddings_norm\n",
    "idx = 4\n",
    "\n",
    "target_embedding = embeddings[idx]\n",
    "\n",
    "print(df_train.iloc[idx]['title'])\n",
    "\n",
    "scores = target_embedding @ embeddings.T\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "top_k_scores = scores[top_k_indices]\n",
    "\n",
    "top_k_titles = df_train.iloc[top_k_indices]['title'].tolist()\n",
    "\n",
    "top_k_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "df_train_deduplicated = df_train.dropna(subset=['title_qwen3_8b_emb', 'full_text_qwen3_8b_emb', 'description_qwen3_8b_emb']).reset_index(drop=True)\n",
    "\n",
    "title_embeddings = np.vstack(df_train_deduplicated['title_qwen3_8b_emb'].values)\n",
    "full_text_embeddings = np.vstack(df_train_deduplicated['full_text_qwen3_8b_emb'].values)\n",
    "text_embeddings = np.vstack(df_train_deduplicated['description_qwen3_8b_emb'].values)\n",
    "\n",
    "\n",
    "embeddings = np.concatenate([title_embeddings*0.8, text_embeddings*0.2], axis=1)\n",
    "embeddings_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / embeddings_norm\n",
    "\n",
    "symmetrized_similarities = embeddings @ embeddings.T\n",
    "\n",
    "af = AffinityPropagation(affinity='precomputed', random_state=0) # Use 'precomputed'\n",
    "af.fit(symmetrized_similarities)\n",
    "labels = af.labels_\n",
    "n_clusters_ = len(af.cluster_centers_indices_)\n",
    "print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "\n",
    "\n",
    "D = 1.01 - symmetrized_similarities\n",
    "np.fill_diagonal(D, 0)\n",
    "score = silhouette_score(D, labels, metric='precomputed')\n",
    "print(score)\n",
    "# print out center examplars of the ap model\n",
    "titles = []\n",
    "for i in range(n_clusters_):\n",
    "    titles.append(df_train_deduplicated.iloc[af.cluster_centers_indices_[i]]['title'])\n",
    "titles.sort()\n",
    "titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Affinity propagation clustering with xgboost features\n",
    "- get the xboost model: \n",
    "    - first generate the training data using: same_occupation_job_pair_sampling.py\n",
    "    - then to train the xgboost model, run: same_occupation_classification.py\n",
    "- to get the clustering results, run clustering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "TMP_DIR = Path(\"../tmp\")\n",
    "CLUSTER_DIR = TMP_DIR / \"xgbt_clustering\"\n",
    "ALPHA = 0.8\n",
    "\n",
    "\"\"\"\n",
    "Estimated number of clusters: 117\n",
    "silhouette score: 0.335116\n",
    "\"\"\"\n",
    "#cleaned\n",
    "\"\"\"\n",
    "Estimated number of clusters: 118\n",
    "silhouette score: 0.360974\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 get training cluster id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen 8b\n",
    "model_path = CLUSTER_DIR / \"palestine_ap_model_qwen3_8b.pkl\"\n",
    "af = pickle.load(open(model_path, 'rb'))\n",
    "    \n",
    "embeddings = np.concatenate([np.vstack(df_train['title_qwen3_8b_emb'].values)*ALPHA, np.vstack(df_train['description_qwen3_8b_emb'].values)*(1-ALPHA)], axis=1)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "exemplars = embeddings[cluster_centers_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k: v for k, v in enumerate(af.labels_)}\n",
    "print(len(id2label))\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a mapping from cluster labels to lists of IDs\n",
    "cluster2ids = defaultdict(list)\n",
    "for id_, cluster in id2label.items():\n",
    "    cluster2ids[cluster].append(id_)\n",
    "\n",
    "# Convert defaultdict to regular dict if needed\n",
    "cluster2ids = dict(cluster2ids)\n",
    "\n",
    "# Print the number of clusters and an example\n",
    "print(f\"Number of clusters: {len(cluster2ids)}\")\n",
    "print(f\"Example cluster contents (first cluster): {list(cluster2ids.values())[0][:5]}\")  # Show first 5 IDs of first cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cluster2ids\n",
    "pickle.dump(cluster2ids, open(CLUSTER_DIR / 'palestine_cluster2id_mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Annotation with LLM (gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from LLM_annotation import annotate_occupations\n",
    "from get_prompts import get_truncated_prompts\n",
    "from prompts import instruction_v4 # arabic aware\n",
    "\n",
    "client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_embed = pd.read_pickle('../data/palestine_train_data_embedding.pkl')\n",
    "\n",
    "# rename title to job_title\n",
    "df_train_embed.rename(columns={'title': 'job_title'}, inplace=True)\n",
    "\n",
    "# get prompts\n",
    "prompts = get_truncated_prompts(df_train_embed, root=None, k=15, use_mmr=True, lambda_param=0.5, instruction=instruction_v4, column_description='job_description_full', use_mapping='../tmp/xgbt_clustering/palestine_cluster2id_mapping.pkl')\n",
    "\n",
    "# save prompts\n",
    "pickle.dump(prompts, open('../tmp/palestine_prompts_118.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotate_occupations(prompts, 'palestine_gemini25_occupations_118.pkl', client, model=\"gemini-2.5-flash-preview-05-20\", temperature=0., max_tokens=4000, provider=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def robust_json_parser(text: str):\n",
    "    \"\"\"\n",
    "    Extracts a JSON object from a string that might be wrapped in markdown code fences\n",
    "    or other text, and parses it.\n",
    "\n",
    "    Args:\n",
    "        text: The input string containing a JSON object.\n",
    "\n",
    "    Returns:\n",
    "        The parsed Python dictionary or list if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    # This regex finds a string that starts with '{' and ends with '}',\n",
    "    # and captures everything in between. re.DOTALL makes '.' match newlines.\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    \n",
    "    if not match:\n",
    "        # If no JSON object is found, try to parse the whole string\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: The string does not contain a valid JSON object.\")\n",
    "            return None\n",
    "\n",
    "    json_str = match.group(0)\n",
    "    \n",
    "    try:\n",
    "        # Parse the extracted string\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        # The extracted string is not valid JSON\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse results\n",
    "\n",
    "for i in range(len(results)):\n",
    "    parsed_data = robust_json_parser(results[i]['annotation'])\n",
    "    if parsed_data:\n",
    "        results[i]['annotation'] = parsed_data\n",
    "    else:\n",
    "        print(f\"Failed to parse annotation for job {results[i]['job_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "pickle.dump(results, open('palestine_gemini25_occupations_118.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. normalize the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import nltk # For lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet # For POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NLTK Setup (run once if not already downloaded) ---\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger.zip')\n",
    "except:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "except:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunctions = []\n",
    "gids = []\n",
    "misc = []\n",
    "for k, v in results.items():\n",
    "    if isinstance(v['annotation'], list):\n",
    "        title = v['annotation'][0]['occupation_title']\n",
    "    elif isinstance(v['annotation'], dict):\n",
    "        title = v['annotation']['occupation_title']\n",
    "    if '+' in title:\n",
    "        conjunctions.append(title)\n",
    "        gids.append(k)\n",
    "    if 'MISC' in title:\n",
    "        misc.append(title)\n",
    "print(len(conjunctions))\n",
    "for c in conjunctions:\n",
    "    print(c)\n",
    "print('-'*100)\n",
    "print(len(misc))\n",
    "for m in misc:\n",
    "    print(m)\n",
    "\n",
    "pickle.dump(gids, open('palestine_conjunction_cluster_ids.pkl', 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idmapping = pickle.load(open('../tmp/xgbt_clustering/palestine_cluster2id_mapping.pkl', 'rb'))\n",
    "\n",
    "conjunction_data = []\n",
    "for id in gids:\n",
    "    conjunction_data.extend(list(idmapping[id]))\n",
    "\n",
    "# save the conjunction data\n",
    "pickle.dump(conjunction_data, open('palestine_conjunction_data_ids.pkl', 'wb'))\n",
    "\n",
    "# remove df train data\n",
    "df_train_embed = df_train_embed[~df_train_embed.index.isin(conjunction_data)]\n",
    "print(len(df_train_embed))\n",
    "df_train_embed.to_pickle('palestine_train_data_no_conjunctions.pkl')\n",
    "\n",
    "# remove results\n",
    "results_keep = {k:v for k,v in results.items() if k not in gids}\n",
    "print(len(results_keep))\n",
    "\n",
    "all_titles = []\n",
    "for k, v in results_keep.items():\n",
    "    if isinstance(v['annotation'], list):\n",
    "        original_title = v['annotation'][0]['occupation_title']\n",
    "        all_titles.append(original_title)\n",
    "    elif isinstance(v['annotation'], dict):\n",
    "        title = v['annotation']['occupation_title']\n",
    "        all_titles.append(title)\n",
    "print(len(set(all_titles)),len(all_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExaminerAgent:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        # Pre-compile regex for efficiency if called many times\n",
    "        self.camel_case_splitter = re.compile(r'(?<=[a-z0-9])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])')\n",
    "        self.punctuation_remover = re.compile(r'[^\\w\\s/-]')  # Keeps words, spaces, hyphens, and slashes\n",
    "        self.multiple_space_reducer = re.compile(r'\\s+')        \n",
    "\n",
    "    def _get_wordnet_pos(self, word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN) # Default to noun\n",
    "\n",
    "    def normalize_title(self, title):\n",
    "        if not title or not isinstance(title, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # 1. Insert spaces before uppercase letters in CamelCase/PascalCase\n",
    "        # e.g., \"OperationsCoordinator\" -> \"Operations Coordinator\"\n",
    "        # e.g., \"PDFReader\" -> \"PDF Reader\"\n",
    "        # e.g., \"MyAPITool\" -> \"My API Tool\"\n",
    "        title_spaced = self.camel_case_splitter.sub(r' ', title)\n",
    "\n",
    "        # 2. Convert to lowercase\n",
    "        title_lower = title_spaced.lower()\n",
    "        \n",
    "        # 3. Remove possessive 's\n",
    "        title_no_possessive = title_lower.replace(\"'s\", \"\")\n",
    "        \n",
    "        # 4. Remove common punctuation (keeps hyphens as they can be significant)\n",
    "        title_no_punct = self.punctuation_remover.sub('', title_no_possessive)\n",
    "        \n",
    "        # 5. Lemmatize\n",
    "        tokens = word_tokenize(title_no_punct) # Tokenize after most cleaning\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, self._get_wordnet_pos(token)) for token in tokens if token.strip()] # ensure no empty tokens\n",
    "\n",
    "        # 6. Join tokens and standardize multiple spaces to single, strip ends\n",
    "        normalized = ' '.join(lemmatized_tokens)\n",
    "        normalized = self.multiple_space_reducer.sub(' ', normalized).strip()\n",
    "        \n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_title(title):\n",
    "    skip_words = {'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', \n",
    "                'to', 'from', 'by', 'with', 'in', 'of'}\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        return title\n",
    "    # Always capitalize the first word\n",
    "    title_case = [words[0].capitalize()]\n",
    "    # For remaining words, capitalize unless they're in skip_words\n",
    "    for word in words[1:]:\n",
    "        if word not in skip_words:\n",
    "            title_case.append(word.capitalize())\n",
    "        else:\n",
    "            title_case.append(word)\n",
    "    return ' '.join(title_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "examiner = ExaminerAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_input = []\n",
    "init_input_with_gid = []\n",
    "seen_titles = set()\n",
    "cluster2normalized_map = {}\n",
    "original2normalized_map = {}\n",
    "for k, v in results_keep.items():\n",
    "    if isinstance(v['annotation'], list):\n",
    "        original_title = v['annotation'][0]['occupation_title']\n",
    "        title = examiner.normalize_title(original_title)\n",
    "        title = capitalize_title(title)\n",
    "        original2normalized_map[original_title] = title\n",
    "        description = v['annotation'][0]['occupation_description']\n",
    "    elif isinstance(v['annotation'], dict):\n",
    "        title = v['annotation']['occupation_title']\n",
    "        original_title = title\n",
    "        title = examiner.normalize_title(title)\n",
    "        title = capitalize_title(title)\n",
    "        original2normalized_map[original_title] = title\n",
    "        description = v['annotation']['occupation_description']\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected annotation type: {type(v['annotation'])}\")\n",
    "    cluster2normalized_map[k] = title\n",
    "    if title in seen_titles:\n",
    "        continue\n",
    "\n",
    "    init_input.append({'title':title, 'description':description})\n",
    "    init_input_with_gid.append({'title':title, 'description':description, 'gid':k})\n",
    "    seen_titles.add(title)\n",
    "len(seen_titles), len(original2normalized_map), len(cluster2normalized_map), len(init_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "for i, d in enumerate(init_input):\n",
    "    title = d['title']\n",
    "    all_titles.append(title)\n",
    "for i in sorted(all_titles):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(original2normalized_map, open('palestine_original2normalized_map.pkl', 'wb'))\n",
    "pickle.dump(cluster2normalized_map, open('palestine_cluster2normalized_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_items = pd.DataFrame(init_input)\n",
    "df_original_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the cleaned nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the description\n",
    "def get_embeddings(text, model, tokenizer, device='cuda:1'):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=4096)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    # Move output back to CPU for numpy conversion\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for description\n",
    "model_name = 'Qwen/Qwen3-Embedding-8B'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = model.to('cuda:1')\n",
    "\n",
    "batch_size = 32\n",
    "title_embeddings = []\n",
    "description_embeddings = []\n",
    "full_text_embeddings = []\n",
    "for i in tqdm(range(0, len(df_original_items), batch_size)):\n",
    "    batch = df_original_items.iloc[i:i+batch_size]\n",
    "                            \n",
    "    batch_titles = batch['title'].tolist()\n",
    "    batch_embeddings_title = [get_embeddings(text, model, tokenizer) for text in batch_titles]\n",
    "    title_embeddings.extend(batch_embeddings_title)\n",
    "\n",
    "    batch_descriptions = batch['description'].tolist()\n",
    "    batch_embeddings_description = [get_embeddings(text, model, tokenizer) for text in batch_descriptions]\n",
    "    description_embeddings.extend(batch_embeddings_description)\n",
    "\n",
    "    batch_full_texts = [title + '\\n' + description for title, description in zip(batch['title'], batch['description'])]\n",
    "    batch_embeddings_full_texts = [get_embeddings(text, model, tokenizer) for text in batch_full_texts]\n",
    "    full_text_embeddings.extend(batch_embeddings_full_texts)\n",
    "df_original_items['title_embeddings'] = title_embeddings\n",
    "df_original_items['description_embeddings'] = description_embeddings\n",
    "df_original_items['full_text_embeddings'] = full_text_embeddings\n",
    "df_original_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_titles = df_original_items['title'].tolist()\n",
    "description_embeddings = np.vstack(df_original_items['description_embeddings'].values)\n",
    "title_embeddings = np.vstack(df_original_items['title_embeddings'].values)\n",
    "embeddings = np.concatenate([description_embeddings, title_embeddings], axis=1)\n",
    "print(f\"Calculating similarity matrix for Affinity Propagation...\")\n",
    "similarity_matrix = sk_cosine_similarity(embeddings)\n",
    "\n",
    "# --- Clustering with Affinity Propagation ---\n",
    "PREFERENCE = 0.95\n",
    "DAMPING = 0.7\n",
    "RANDOM_STATE = 42 # For reproducibility\n",
    "\n",
    "\n",
    "ap_model = AffinityPropagation(\n",
    "    # damping=DAMPING,\n",
    "    preference=PREFERENCE,\n",
    "    affinity='precomputed',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "ap_model.fit(similarity_matrix)\n",
    "\n",
    "cluster_labels_for_valid = ap_model.labels_\n",
    "exemplar_indices_in_valid = ap_model.cluster_centers_indices_ # Indices within the 'valid_descriptions' list\n",
    "\n",
    "n_clusters_ = len(exemplar_indices_in_valid)\n",
    "print(f'Estimated number of clusters from Affinity Propagation: {n_clusters_}')\n",
    "\n",
    "# Create a mapping from cluster label to the canonical title (title of the exemplar)\n",
    "canonical_titles_map = {}\n",
    "for i, exemplar_idx_in_valid in enumerate(exemplar_indices_in_valid):\n",
    "    # exemplar_idx_in_valid is an index into 'description_embeddings' / 'original_titles_valid'\n",
    "    # The cluster label assigned by AP for this exemplar's cluster will be the exemplar's own index\n",
    "    # in 'description_embeddings' IF it's an exemplar.\n",
    "    # More robustly, AP assigns labels from 0 to n_clusters-1.\n",
    "    # We need to map which cluster label corresponds to which exemplar.\n",
    "    # The `ap_model.labels_` gives the cluster label for each point.\n",
    "    # The `exemplar_indices_in_valid` gives the index of the exemplar FOR EACH CLUSTER.\n",
    "    # A simpler way: ap_model.labels_[exemplar_idx_in_valid] gives the cluster label for that exemplar.\n",
    "    # We want: cluster_label -> title_of_exemplar_for_that_cluster\n",
    "    \n",
    "    # Let's find which cluster label corresponds to this exemplar\n",
    "    # The label of the cluster whose exemplar is at `exemplar_idx_in_valid`\n",
    "    # is simply `ap_model.labels_[exemplar_idx_in_valid]`\n",
    "    \n",
    "    cluster_label_of_exemplar = ap_model.labels_[exemplar_idx_in_valid]\n",
    "    canonical_titles_map[cluster_label_of_exemplar] = original_titles[exemplar_idx_in_valid]\n",
    "\n",
    "\n",
    "# Map cluster labels back to all original items\n",
    "all_cluster_labels = np.full(len(original_titles), -1, dtype=int) # Default to -1 (unclustered/error)\n",
    "final_canonical_titles = [None] * len(original_titles)\n",
    "\n",
    "for i in range(len(original_titles)):\n",
    "    original_item_cluster_label = cluster_labels_for_valid[i]\n",
    "    all_cluster_labels[i] = original_item_cluster_label\n",
    "    if original_item_cluster_label in canonical_titles_map:\n",
    "        final_canonical_titles[i] = canonical_titles_map[original_item_cluster_label]\n",
    "    else:\n",
    "        print(f\"Cluster label {original_item_cluster_label} not found in canonical_titles_map\")\n",
    "        # This case should ideally not happen if clustering is successful\n",
    "        # but as a fallback, use its own title if its cluster exemplar is not found (e.g. -1 label from fit)\n",
    "        final_canonical_titles[i] = original_titles[i]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "canonical2original_map = defaultdict(list)\n",
    "for i, v in enumerate(final_canonical_titles):\n",
    "    canonical2original_map[v].append(original_titles[i])\n",
    "for k, v in canonical2original_map.items():\n",
    "    if len(v) > 1:\n",
    "        print(k)\n",
    "        print(v)\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(canonical2original_map, open('palestine_canonical2normalized_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original2canonical_map\n",
    "original2canonical_map = {}\n",
    "for k, v in canonical2original_map.items():\n",
    "    for vv in v:\n",
    "        original2canonical_map[vv] = k\n",
    "len(original2canonical_map)\n",
    "pickle.dump(original2canonical_map, open('palestine_normalized2canonical_map.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the original titles to the canonical titles\n",
    "df_original_items['canonical_title'] = final_canonical_titles\n",
    "# save the dataframe\n",
    "df_original_items.to_pickle('palestine_init_input_test_full_with_canonical_title_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2canonical_map = {}\n",
    "for k, v in cluster2normalized_map.items():\n",
    "    cluster2canonical_map[k] = original2canonical_map[v]\n",
    "len(cluster2canonical_map)\n",
    "pickle.dump(cluster2canonical_map, open('palestine_cluster2canonical_map.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input format\n",
    "new_input = []\n",
    "title2description = {}\n",
    "for item in init_input:\n",
    "    title2description[item['title']] = item['description']\n",
    "for k in canonical2original_map.keys():\n",
    "    new_input.append({'title': k, 'description': title2description[k]})\n",
    "len(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(new_input, open('palestine_init_input_test_full_cannonical.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Build taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input: palestine_init_input_test_full_cannonical.json -> tree_multiagent.py -> output: full_generated_taxonomy_with_desc.json, level*_cannonical_parents_with_desc.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. convert to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tmp/palestine_taxonomy_output/full_generated_taxonomy_with_desc.json', 'r') as f:\n",
    "    json_string = f.read()\n",
    "taxonomy_data_loaded = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_taxonomy_to_tree_format(taxonomy_data_loaded):\n",
    "    \"\"\"\n",
    "    Convert taxonomy data from the original format to the desired tree format.\n",
    "    Uses the title as the node name.\n",
    "    \n",
    "    Args:\n",
    "        taxonomy_data_loaded: Dictionary with level numbers as keys and lists of taxonomy items as values\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary representing the root node of the tree in the desired format\n",
    "    \"\"\"\n",
    "    # Create a mapping from title to node at each level\n",
    "    title_to_node = {}\n",
    "    \n",
    "    # Process levels from highest to lowest (e.g., 4, 3, 2, 1, 0)\n",
    "    max_level = max(int(k) for k in taxonomy_data_loaded.keys())\n",
    "    \n",
    "    # Start with the root node\n",
    "    root_node = {\n",
    "        \"parent\": \"null\",\n",
    "        \"name\": \"Root\",  # You can customize this root name\n",
    "        \"edge_name\": \"null\",\n",
    "        \"children\": []\n",
    "    }\n",
    "    \n",
    "    # Process each level starting from max_level down to 1\n",
    "    for level in range(max_level, 0, -1):\n",
    "        level_str = str(level)\n",
    "        if level_str not in taxonomy_data_loaded:\n",
    "            continue\n",
    "            \n",
    "        level_items = taxonomy_data_loaded[level_str]\n",
    "        \n",
    "        for i, item in enumerate(level_items):\n",
    "            title = item['title']\n",
    "            \n",
    "            # Create a node for this item\n",
    "            node = {\n",
    "                \"parent\": None,  # Will be set later\n",
    "                \"name\": title,\n",
    "                \"edge_name\": title,\n",
    "                \"children\": []\n",
    "            }\n",
    "            \n",
    "            # If this is the top level, attach directly to root\n",
    "            if level == max_level:\n",
    "                node[\"parent\"] = root_node[\"name\"]\n",
    "                root_node[\"children\"].append(node)\n",
    "            else:\n",
    "                # Find a parent for this node\n",
    "                # We'll look for a parent that has this title in its 'kids' list\n",
    "                found_parent = False\n",
    "                for potential_parent_level in range(level + 1, max_level + 1):\n",
    "                    parent_level_str = str(potential_parent_level)\n",
    "                    if parent_level_str not in taxonomy_data_loaded:\n",
    "                        continue\n",
    "                        \n",
    "                    for parent_item in taxonomy_data_loaded[parent_level_str]:\n",
    "                        if 'kids' in parent_item and title in parent_item['kids']:\n",
    "                            parent_node = title_to_node[parent_item['title']]\n",
    "                            node[\"parent\"] = parent_node[\"name\"]\n",
    "                            parent_node[\"children\"].append(node)\n",
    "                            found_parent = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_parent:\n",
    "                        break\n",
    "                \n",
    "                # If no parent found, attach to root (this is a fallback)\n",
    "                if not found_parent:\n",
    "                    node[\"parent\"] = root_node[\"name\"]\n",
    "                    root_node[\"children\"].append(node)\n",
    "            \n",
    "            # Store this node for future reference\n",
    "            title_to_node[title] = node\n",
    "    \n",
    "    # Process level 0 (leaf nodes) if available\n",
    "    if '0' in taxonomy_data_loaded:\n",
    "        for item in taxonomy_data_loaded['0']['Leaf Items']:\n",
    "            title = item['title']\n",
    "            leaf_node = {\n",
    "                \"parent\": None,\n",
    "                \"name\": title,\n",
    "                \"edge_name\": \"\",  # Using the format from your example\n",
    "                \"children\": []\n",
    "            }\n",
    "            \n",
    "            # Find a parent for this leaf node\n",
    "            found_parent = False\n",
    "            for level in range(1, max_level + 1):\n",
    "                level_str = str(level)\n",
    "                if level_str not in taxonomy_data_loaded:\n",
    "                    continue\n",
    "                    \n",
    "                for parent_item in taxonomy_data_loaded[level_str]:\n",
    "                    if 'kids' in parent_item and title in parent_item['kids']:\n",
    "                        parent_node = title_to_node[parent_item['title']]\n",
    "                        leaf_node[\"parent\"] = parent_node[\"name\"]\n",
    "                        parent_node[\"children\"].append(leaf_node)\n",
    "                        found_parent = True\n",
    "                        break\n",
    "                \n",
    "                if found_parent:\n",
    "                    break\n",
    "            \n",
    "            # If no parent found, attach to root (this is a fallback)\n",
    "            if not found_parent:\n",
    "                leaf_node[\"parent\"] = root_node[\"name\"]\n",
    "                root_node[\"children\"].append(leaf_node)\n",
    "    \n",
    "    return root_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = convert_taxonomy_to_tree_format(taxonomy_data_loaded)\n",
    "# save the result\n",
    "# Write the output tree to a JSON file.\n",
    "with open(\"../docs/palestine_Gemini_generated_tree_91.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
